{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"use_model.ipynb","version":"0.3.2","provenance":[{"file_id":"1uR180q-eeDiSudfdoK43cYt2ZZ-77b-Q","timestamp":1555068738649}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"MdSym62YUNrj","colab_type":"code","outputId":"c5206d2b-4b73-4290-ec59-b9a3ea83e163","executionInfo":{"status":"ok","timestamp":1556523589336,"user_tz":-480,"elapsed":2520,"user":{"displayName":"Jeesup Kim","photoUrl":"","userId":"00951430684975732562"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer #stemmer\n","\n","import re\n","import string\n","from bs4 import BeautifulSoup\n","import pickle"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"metadata":{"id":"ilnaFgO5UT8N","colab_type":"code","outputId":"35174b2c-32cc-4a10-bda6-62b22430f2df","executionInfo":{"status":"ok","timestamp":1556525090869,"user_tz":-480,"elapsed":1115,"user":{"displayName":"Jeesup Kim","photoUrl":"","userId":"00951430684975732562"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")  \n","\n","import sys\n","import time\n","\n","from sklearn.feature_extraction.text import CountVectorizer #For Bag of words\n","from sklearn.feature_extraction.text import TfidfVectorizer #For TF-IDF\n","from gensim.models import Word2Vec                          #For Word2Vec\n","\n","from sklearn.model_selection import train_test_split\n","from keras.utils import np_utils\n","\n","###\n","\n","import itertools\n","import os\n","\n","from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n","\n","from keras.models import Sequential, Model, load_model\n","\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","\n","from keras.layers import Input, Dense, Activation, Dropout, LSTM, Flatten\n","from keras.layers import Embedding\n","from keras.layers import Conv1D, GlobalMaxPooling1D\n","from keras.preprocessing import text, sequence\n","from keras import utils"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"8IT8U0_rUUmw","colab_type":"code","colab":{}},"cell_type":"code","source":["def load_model(file_path):\n","  model = keras.models.load_model(file_path)\n","  return model\n","\n","def predict_class(input_x, model):\n","  y_probs = model.predict(input_x) \n","  y_classes = y_probs.argmax(axis=-1)\n","  return y_probs\n","\n","uri_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n","\n","def stripTagsAndUris(x):\n","    if x:\n","        # BeautifulSoup on content\n","        soup = BeautifulSoup(x, \"html.parser\")\n","        # Stripping all <code> tags with their content if any\n","        if soup.code:\n","            soup.code.decompose()\n","        # Get all the text out of the html\n","        text =  soup.get_text()\n","        # Returning text stripping out all uris\n","        return re.sub(uri_re, \"\", text)\n","    else:\n","        return \"\"\n","\n","def removePunctuation(x):\n","    # Lowercasing all words\n","    x = x.lower()\n","    # Removing non ASCII chars\n","    x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n","    # Removing (replacing with empty spaces actually) all the punctuations\n","    return re.sub(\"[\"+string.punctuation+\"]\", \" \", x)\n","\n","snow = nltk.stem.SnowballStemmer('english')\n","stops = set(stopwords.words(\"english\"))\n","def stemAndRemoveStopwords(x):\n","    # Removing all the stopwords\n","    filtered_words = [snow.stem(word) for word in x.split() if word not in stops]\n","    return \" \".join(filtered_words)\n","    \n","def remove_pattern(input_txt, pattern):\n","    r = re.findall(pattern, input_txt)\n","    for i in r:\n","        input_txt = re.sub(i, '', input_txt)    \n","    return input_txt\n","  \n","def preprocess(df):\n","    df[\"content\"] = np.vectorize(remove_pattern)(df[\"content\"], \"@[\\w]*\")\n","    df[\"content\"] = df[\"content\"].map(stripTagsAndUris)\n","    df[\"content\"] = df[\"content\"].map(removePunctuation)\n","    df[\"content\"] = df[\"content\"].map(stemAndRemoveStopwords)\n","    return df\n","  \n","def insert_text(input_text, dataframe):\n","    dataframe = dataframe.append({'content' : input_text}, ignore_index=True)\n","    return dataframe"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i8sPsyGwYVos","colab_type":"code","outputId":"0539bff3-8b82-475b-8948-fc7645c800a6","executionInfo":{"status":"ok","timestamp":1556525095697,"user_tz":-480,"elapsed":937,"user":{"displayName":"Jeesup Kim","photoUrl":"","userId":"00951430684975732562"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"cell_type":"code","source":["df = pd.DataFrame(columns=['content', 'sentiment'])\n","print(df)\n","df = insert_text(\"I see trees of green, red roses too\", df)\n","df = insert_text(\"I see them bloom for me and you\", df)\n","df = insert_text(\"And I think to myself what a wonderful world\", df)\n","df = insert_text(\"I see skies of blue and clouds of white\", df)\n","df = insert_text(\"The bright blessed day, the dark sacred night\", df)\n","df = insert_text(\"And I think to myself what a wonderful world\", df)\n","print(df)\n","print()\n","df = preprocess(df)\n","print(df)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Empty DataFrame\n","Columns: [content, sentiment]\n","Index: []\n","                                         content sentiment\n","0            I see trees of green, red roses too       NaN\n","1                I see them bloom for me and you       NaN\n","2   And I think to myself what a wonderful world       NaN\n","3        I see skies of blue and clouds of white       NaN\n","4  The bright blessed day, the dark sacred night       NaN\n","5   And I think to myself what a wonderful world       NaN\n","\n","                            content sentiment\n","0           see tree green red rose       NaN\n","1                         see bloom       NaN\n","2                think wonder world       NaN\n","3          see sky blue cloud white       NaN\n","4  bright bless day dark sacr night       NaN\n","5                think wonder world       NaN\n"],"name":"stdout"}]},{"metadata":{"id":"DF36udu0GqmW","colab_type":"code","outputId":"e2b202cd-64cb-4858-a3c1-c94e0cdcf82f","executionInfo":{"status":"ok","timestamp":1556527564263,"user_tz":-480,"elapsed":7199,"user":{"displayName":"Jeesup Kim","photoUrl":"","userId":"00951430684975732562"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"cell_type":"code","source":["model = load_model(\"../content/drive/My Drive/Colab Notebooks/models/model-ngram.h5\")\n","print(df['content'][0])\n","test_text = df['content']\n","picklefile = open(\"../content/drive/My Drive/Colab Notebooks/models/tfidf.pickle\", 'rb')\n","tfidfvectorizer = pickle.load(picklefile)\n","# tokenize = text.Tokenizer(num_words=11231, char_level=False)\n","\n","x_text = tfidfvectorizer.transform(test_text.values.astype('U'))\n","  \n","# print(x_text)\n","predictions= predict_class(x_text,model)\n","print(predictions.shape)\n","print(test_text, \"\\n\", predictions)\n","print(np.argmax(predictions, axis=1))"],"execution_count":35,"outputs":[{"output_type":"stream","text":["see tree green red rose\n","(6, 4)\n","0             see tree green red rose\n","1                           see bloom\n","2                  think wonder world\n","3            see sky blue cloud white\n","4    bright bless day dark sacr night\n","5                  think wonder world\n","Name: content, dtype: object \n"," [[0.1112662  0.36074355 0.2946531  0.23333721]\n"," [0.13024132 0.23749384 0.4181508  0.21411401]\n"," [0.04955224 0.47362596 0.28467306 0.19214877]\n"," [0.05905165 0.13261427 0.22041218 0.587922  ]\n"," [0.01125506 0.6135496  0.13459261 0.24060275]\n"," [0.04955224 0.47362596 0.28467304 0.19214876]]\n","[1 2 1 3 1 1]\n"],"name":"stdout"}]}]}