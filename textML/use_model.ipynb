{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"use_model.ipynb","version":"0.3.2","provenance":[{"file_id":"1uR180q-eeDiSudfdoK43cYt2ZZ-77b-Q","timestamp":1555068738649}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"MdSym62YUNrj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"2231cea6-75bd-436a-9432-e7cb8d0ed84f","executionInfo":{"status":"ok","timestamp":1555072153547,"user_tz":-480,"elapsed":807,"user":{"displayName":"Jeesup Kim","photoUrl":"","userId":"00951430684975732562"}}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer #stemmer\n","\n","import re\n","import string\n","from bs4 import BeautifulSoup"],"execution_count":34,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"metadata":{"id":"ilnaFgO5UT8N","colab_type":"code","colab":{}},"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")  \n","\n","import sys\n","import time\n","\n","from sklearn.feature_extraction.text import CountVectorizer #For Bag of words\n","from sklearn.feature_extraction.text import TfidfVectorizer #For TF-IDF\n","from gensim.models import Word2Vec                          #For Word2Vec\n","\n","from sklearn.model_selection import train_test_split\n","from keras.utils import np_utils\n","\n","###\n","\n","import itertools\n","import os\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","\n","from keras.models import Sequential, Model, load_model\n","\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","\n","from keras.layers import Input, Dense, Activation, Dropout, LSTM, Flatten\n","from keras.layers import Embedding\n","from keras.layers import Conv1D, GlobalMaxPooling1D\n","from keras.preprocessing import text, sequence\n","from keras import utils"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8IT8U0_rUUmw","colab_type":"code","colab":{}},"cell_type":"code","source":["def load_model(file_path):\n","  model = keras.models.load_model(file_path)\n","  return model\n","\n","def predict_class(input_x, model):\n","  y_probs = model.predict(input_x) \n","  y_classes = y_probs.argmax(axis=-1)\n","  return y_probs\n","\n","uri_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n","\n","def stripTagsAndUris(x):\n","    if x:\n","        # BeautifulSoup on content\n","        soup = BeautifulSoup(x, \"html.parser\")\n","        # Stripping all <code> tags with their content if any\n","        if soup.code:\n","            soup.code.decompose()\n","        # Get all the text out of the html\n","        text =  soup.get_text()\n","        # Returning text stripping out all uris\n","        return re.sub(uri_re, \"\", text)\n","    else:\n","        return \"\"\n","\n","def removePunctuation(x):\n","    # Lowercasing all words\n","    x = x.lower()\n","    # Removing non ASCII chars\n","    x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n","    # Removing (replacing with empty spaces actually) all the punctuations\n","    return re.sub(\"[\"+string.punctuation+\"]\", \" \", x)\n","\n","snow = nltk.stem.SnowballStemmer('english')\n","stops = set(stopwords.words(\"english\"))\n","def stemAndRemoveStopwords(x):\n","    # Removing all the stopwords\n","    filtered_words = [snow.stem(word) for word in x.split() if word not in stops]\n","    return \" \".join(filtered_words)\n","    \n","def remove_pattern(input_txt, pattern):\n","    r = re.findall(pattern, input_txt)\n","    for i in r:\n","        input_txt = re.sub(i, '', input_txt)    \n","    return input_txt\n","  \n","def preprocess(df):\n","    df[\"content\"] = np.vectorize(remove_pattern)(df[\"content\"], \"@[\\w]*\")\n","    df[\"content\"] = df[\"content\"].map(stripTagsAndUris)\n","    df[\"content\"] = df[\"content\"].map(removePunctuation)\n","    df[\"content\"] = df[\"content\"].map(stemAndRemoveStopwords)\n","    return df\n","  \n","def insert_text(input_text, dataframe):\n","    dataframe = dataframe.append({'content' : input_text}, ignore_index=True)\n","    return dataframe"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i8sPsyGwYVos","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":612},"outputId":"e574cb62-f860-4474-fc77-183d628fbb10","executionInfo":{"status":"ok","timestamp":1555072409411,"user_tz":-480,"elapsed":2720,"user":{"displayName":"Jeesup Kim","photoUrl":"","userId":"00951430684975732562"}}},"cell_type":"code","source":["df = pd.DataFrame(columns=['content', 'sentiment'])\n","print(df)\n","df = insert_text(\"I had great lunch today I feel good\", df)\n","print(df)\n","df = preprocess(df)\n","print(df)\n","model = load_model(\"../content/drive/My Drive/Colab Notebooks/models/model-simple.h5\")\n","print(df['content'][0])\n","\n","tokenize = text.Tokenizer(num_words=3000, char_level=False)\n","x_text = tokenize.texts_to_matrix(df['content'][0])\n","# angry, happy, neutral, sad\n","# 1. 0. 0. 0. \n","# 0. 1. 0. 0.\n","# 0. 0. 1. 0.\n","# 0. 0. 0. 1.\n","print(predict_class(x_text, model))"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Empty DataFrame\n","Columns: [content, sentiment]\n","Index: []\n","                               content sentiment\n","0  I had great lunch today I feel good       NaN\n","                       content sentiment\n","0  great lunch today feel good       NaN\n","great lunch today feel good\n","[[0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.21192788 0.14467524 0.4894602  0.15393667]\n"," [0.2119279  0.14467525 0.4894602  0.15393667]\n"," [0.2119279  0.14467525 0.4894602  0.15393667]\n"," [0.2119279  0.14467525 0.4894602  0.15393667]]\n"],"name":"stdout"}]}]}